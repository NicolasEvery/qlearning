{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import mujoco\n",
    "# import json\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "# from DQNAgent import DQNAgent\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Cargar configuración de Q-learning\n",
    "# with open('qlearning.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Parámetros de DQN\n",
    "# max_epsilon = data['max_epsilon']\n",
    "# min_epsilon = data['min_epsilon']\n",
    "# decay_rate = data['decay_rate']\n",
    "# gamma = data['gamma']\n",
    "# learning_rate = data['lr']\n",
    "# memory_size = data['mem_size']\n",
    "# batch_size = data['batchsize']\n",
    "\n",
    "# # Cargar modelo MuJoCo\n",
    "# model_dir = Path(\"trs_so_arm100\")\n",
    "# model_xml = model_dir / \"scene.xml\"\n",
    "\n",
    "# class RoboticArmEnv():\n",
    "#     def __init__(self):\n",
    "#         # Load model.\n",
    "#         self.model = mujoco.MjModel.from_xml_path(str(model_xml))\n",
    "#         self.data = mujoco.MjData(self.model)\n",
    "\n",
    "#         # Espacio de observación (posiciones y velocidades)\n",
    "#         self.state_dim = self.model.nq + self.model.nv  # nq = posiciones, nv = velocidades\n",
    "#         self.action_dim = self.model.nu  # Número de actuadores\n",
    "\n",
    "#         # Discretización de acciones: cada articulación tiene 3 movimientos (-1, 0, 1)\n",
    "#         self.num_actions = 3 ** self.action_dim  # Posibles combinaciones de acciones\n",
    "\n",
    "#         # Agregar renderer para capturar imágenes\n",
    "#         self.renderer = mujoco.Renderer(self.model)\n",
    "\n",
    "#         self.agent = DQNAgent(state_dim=self.state_dim,\n",
    "#                             num_actions=self.num_actions,\n",
    "#                             action_dim=self.action_dim,\n",
    "#                             gamma=gamma,\n",
    "#                             epsilon=max_epsilon,\n",
    "#                             epsilon_min=min_epsilon,\n",
    "#                             epsilon_decay=decay_rate,\n",
    "#                             learning_rate=learning_rate,\n",
    "#                             memory_size=memory_size,\n",
    "#                             batch_size=batch_size)\n",
    "\n",
    "\n",
    "#         self.target_position = np.array([0.7, 1, 1.2, 0.7, 0.2, 0])  # Meta deseada\n",
    "#         self.reset()\n",
    "\n",
    "#     def step(self, action):\n",
    "#         # Convertir acción discreta en torques continuos\n",
    "#         action_values = [(a - 1) * 0.5 for a in action]  # Aplica la conversión a cada valor\n",
    "#         self.data.ctrl[:] = action_values\n",
    "\n",
    "#         mujoco.mj_step(self.model, self.data)\n",
    "\n",
    "#         # Obtener observación\n",
    "#         obs = np.concatenate([self.data.qpos, self.data.qvel])\n",
    "\n",
    "#         # Calcular recompensa basada en distancia\n",
    "#         distance = np.linalg.norm(self.target_position - self.data.qpos)\n",
    "#         reward = -distance  # Penaliza la distancia a la meta\n",
    "\n",
    "#         done = distance < 0.05  # Termina si alcanza la meta\n",
    "#         return obs, reward, done\n",
    "\n",
    "#     def reset(self):\n",
    "#         mujoco.mj_resetData(self.model, self.data)\n",
    "#         return np.concatenate([self.data.qpos, self.data.qvel])\n",
    "\n",
    "#     def train(self, episodes=500):\n",
    "#         success_history = []\n",
    "\n",
    "#         for episode in tqdm(range(episodes)):\n",
    "#             state = self.reset()\n",
    "#             done = False\n",
    "#             total_reward = 0\n",
    "#             x = 0\n",
    "\n",
    "#             while not done:\n",
    "#                 action = self.agent.get_action(state)\n",
    "#                 next_state, reward, done = self.step(action)\n",
    "\n",
    "#                 # Guardar experiencia en memoria\n",
    "#                 self.agent.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "#                 # Entrenar el agente\n",
    "#                 self.agent.replay()\n",
    "\n",
    "#                 state = next_state\n",
    "#                 total_reward += reward\n",
    "\n",
    "#                 # Renderizar y mostrar un frame cada 1000 episodios\n",
    "#                 if x % 10000 == 0 and x > 0:\n",
    "#                     print(f\"Rendering frame at episode {episode}...\")\n",
    "#                     self.renderer.update_scene(self.data)\n",
    "#                     img = self.renderer.render()\n",
    "#                     plt.imshow(img)\n",
    "#                     plt.axis(\"off\")\n",
    "#                     plt.title(f\"Frame at Episode {episode}\")\n",
    "#                     plt.show()\n",
    "#                 x+=1    \n",
    "\n",
    "#             success_history.append(total_reward)\n",
    "\n",
    "#             if episode % 50 == 0:\n",
    "#                 print(f\"Episode {episode}: Reward {total_reward:.2f}\")\n",
    "\n",
    "#         # Guardar modelo entrenado\n",
    "#         torch.save({\n",
    "#             'policy_net_state_dict': self.agent.policy_net.state_dict(),\n",
    "#             'target_net_state_dict': self.agent.target_net.state_dict(),\n",
    "#             'optimizer_state_dict': self.agent.optimizer.state_dict(),\n",
    "#         }, 'Checkpoint/dqn_robotic_arm.pth')\n",
    "\n",
    "#         # Graficar desempeño\n",
    "#         plt.plot(success_history)\n",
    "#         plt.xlabel(\"Episode\")\n",
    "#         plt.ylabel(\"Total Reward\")\n",
    "#         plt.title(\"Training Progress\")\n",
    "#         plt.show()\n",
    "s\n",
    "# # Entrenar el agente\n",
    "# if __name__ == \"__main__\":\n",
    "#     env = RoboticArmEnv()\n",
    "#     env.train(episodes=500)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
